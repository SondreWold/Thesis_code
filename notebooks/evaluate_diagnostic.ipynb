{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report,accuracy_score, matthews_corrcoef, confusion_matrix\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate on Glue Diagnostic, \"Ax\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This scripts compares predictions on the Glue Diagnostic set against the gold labels. The script expects that the predictions are on the format: index\\tpredictions\\n\n",
    "\n",
    "That is:\n",
    "\n",
    "1 contradiction\n",
    "\n",
    "2 entailment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold = get_dataframe(\"../data/diagnostic_gold.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe(path: str) -> pd.DataFrame:\n",
    "    return pd.read_csv(path, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_df(predictions: pd.DataFrame, gold: pd.DataFrame = gold):\n",
    "    final = gold.copy()\n",
    "    final[\"prediction\"] = predictions[\"prediction\"]\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_full(df_gold: pd.DataFrame, predictions_path: str):\n",
    "    predictions = get_dataframe(predictions_path)\n",
    "    final = merge_df(predictions, df_gold)\n",
    "    true_labels = list(final[\"Label\"])\n",
    "    pred_labels = list(final[\"prediction\"])\n",
    "    print(f\"MCC score: {matthews_corrcoef(true_labels, pred_labels)}\")\n",
    "    print(classification_report(true_labels, pred_labels, target_names=[\"contradiction\", \"neutral\", \"entailment\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_common_sense(df_gold: pd.DataFrame, predictions_path: str):\n",
    "    predictions = get_dataframe(predictions_path)\n",
    "    final = merge_df(predictions, df_gold)\n",
    "    \n",
    "    knowledge = final[~final['Knowledge'].isnull()]\n",
    "    true_labels = list(knowledge[\"Label\"])\n",
    "    pred_labels = list(knowledge[\"prediction\"])\n",
    "    \n",
    "    knowledge_common = knowledge[knowledge.Knowledge.str.contains('Common',case=False)]\n",
    "    true_labels = list(knowledge_common[\"Label\"])\n",
    "    pred_labels = list(knowledge_common[\"prediction\"])\n",
    "\n",
    "    print(f\"MCC score: {matthews_corrcoef(true_labels, pred_labels)}\")\n",
    "    print(classification_report(true_labels, pred_labels, target_names=[\"contradiction\", \"neutral\", \"entailment\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_world_knowledge(df_gold: pd.DataFrame, predictions_path: str):\n",
    "    predictions = get_dataframe(predictions_path)\n",
    "    final = merge_df(predictions, df_gold)\n",
    "    \n",
    "    knowledge = final[~final['Knowledge'].isnull()]\n",
    "    true_labels = list(knowledge[\"Label\"])\n",
    "    pred_labels = list(knowledge[\"prediction\"])\n",
    "    \n",
    "    knowledge_world = knowledge[knowledge.Knowledge.str.contains('World',case=False)]\n",
    "    true_labels = list(knowledge_world[\"Label\"])\n",
    "    pred_labels = list(knowledge_world[\"prediction\"])\n",
    "\n",
    "    print(f\"MCC score: {matthews_corrcoef(true_labels, pred_labels)}\")\n",
    "    print(classification_report(true_labels, pred_labels, target_names=[\"contradiction\", \"neutral\", \"entailment\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full diagnostic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bert base uncased, 3e5, 16 batch size, 3 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCC score: 0.3715884275271643\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.56      0.52      0.54       258\n",
      "      neutral       0.63      0.75      0.69       460\n",
      "   entailment       0.56      0.45      0.50       386\n",
      "\n",
      "     accuracy                           0.59      1104\n",
      "    macro avg       0.58      0.58      0.58      1104\n",
      " weighted avg       0.59      0.59      0.59      1104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare_full(gold, \"../diagnostic_results/bert_base_uncased_3e5_16_3/results_ax.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roberta base, 3e5, 16 batch size, 3 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCC score: 0.41188796925134874\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.59      0.55      0.57       258\n",
      "      neutral       0.64      0.78      0.71       460\n",
      "   entailment       0.60      0.47      0.53       386\n",
      "\n",
      "     accuracy                           0.62      1104\n",
      "    macro avg       0.61      0.60      0.60      1104\n",
      " weighted avg       0.62      0.62      0.61      1104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare_full(gold, \"../diagnostic_results/roberta_base_3e5_16_3/results_ax.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Houlsby 100k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCC score: 0.3560530342987728\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.56      0.46      0.50       258\n",
      "      neutral       0.61      0.77      0.68       460\n",
      "   entailment       0.57      0.46      0.51       386\n",
      "\n",
      "     accuracy                           0.59      1104\n",
      "    macro avg       0.58      0.56      0.56      1104\n",
      " weighted avg       0.58      0.59      0.58      1104\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare_full(gold, \"../diagnostic_results/houlsby_100k.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common sense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bert base uncased, 3e5, 16 batch size, 3 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCC score: 0.32741091196998096\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.68      0.52      0.59        58\n",
      "      neutral       0.60      0.50      0.54        56\n",
      "   entailment       0.39      0.64      0.48        36\n",
      "\n",
      "     accuracy                           0.54       150\n",
      "    macro avg       0.56      0.55      0.54       150\n",
      " weighted avg       0.58      0.54      0.55       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare_common_sense(gold, \"../diagnostic_results/bert_base_uncased_3e5_16_3/results_ax.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roberta base, 3e5, 16 batch size, 3 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCC score: 0.3708507737093856\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.65      0.48      0.55        58\n",
      "      neutral       0.62      0.59      0.61        56\n",
      "   entailment       0.46      0.69      0.56        36\n",
      "\n",
      "     accuracy                           0.57       150\n",
      "    macro avg       0.58      0.59      0.57       150\n",
      " weighted avg       0.60      0.57      0.57       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare_common_sense(gold, \"../diagnostic_results/roberta_base_3e5_16_3/results_ax.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Houlsby 100k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCC score: 0.2646749238184288\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.60      0.41      0.49        58\n",
      "      neutral       0.57      0.54      0.55        56\n",
      "   entailment       0.37      0.58      0.45        36\n",
      "\n",
      "     accuracy                           0.50       150\n",
      "    macro avg       0.51      0.51      0.50       150\n",
      " weighted avg       0.53      0.50      0.50       150\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare_common_sense(gold, \"../diagnostic_results/houlsby_100k.tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### World knowledge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bert base uncased, 3e5, 16 batch size, 3 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCC score: 0.13369507292599506\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.23      0.19      0.21        32\n",
      "      neutral       0.66      0.52      0.58        63\n",
      "   entailment       0.33      0.49      0.39        39\n",
      "\n",
      "     accuracy                           0.43       134\n",
      "    macro avg       0.41      0.40      0.39       134\n",
      " weighted avg       0.46      0.43      0.44       134\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare_world_knowledge(gold, \"../diagnostic_results/bert_base_uncased_3e5_16_3/results_ax.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Roberta base, 3e5, 16 batch size, 3 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCC score: 0.14983036699702876\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.24      0.16      0.19        32\n",
      "      neutral       0.66      0.56      0.60        63\n",
      "   entailment       0.33      0.51      0.40        39\n",
      "\n",
      "     accuracy                           0.45       134\n",
      "    macro avg       0.41      0.41      0.40       134\n",
      " weighted avg       0.46      0.45      0.45       134\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare_world_knowledge(gold, \"../diagnostic_results/roberta_base_3e5_16_3/results_ax.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Houlsby 100k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCC score: 0.13022222411185316\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "contradiction       0.28      0.16      0.20        32\n",
      "      neutral       0.65      0.52      0.58        63\n",
      "   entailment       0.31      0.51      0.38        39\n",
      "\n",
      "     accuracy                           0.43       134\n",
      "    macro avg       0.41      0.40      0.39       134\n",
      " weighted avg       0.46      0.43      0.43       134\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare_world_knowledge(gold, \"../diagnostic_results/houlsby_100k.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.7 64-bit",
   "language": "python",
   "name": "python37764bit1943f163d28f4bc9b7721f93b8953027"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
